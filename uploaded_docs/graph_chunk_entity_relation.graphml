<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d9" for="edge" attr.name="file_path" attr.type="string" />
  <key id="d8" for="edge" attr.name="source_id" attr.type="string" />
  <key id="d7" for="edge" attr.name="keywords" attr.type="string" />
  <key id="d6" for="edge" attr.name="description" attr.type="string" />
  <key id="d5" for="edge" attr.name="weight" attr.type="double" />
  <key id="d4" for="node" attr.name="file_path" attr.type="string" />
  <key id="d3" for="node" attr.name="source_id" attr.type="string" />
  <key id="d2" for="node" attr.name="description" attr.type="string" />
  <key id="d1" for="node" attr.name="entity_type" attr.type="string" />
  <key id="d0" for="node" attr.name="entity_id" attr.type="string" />
  <graph edgedefault="undirected">
    <node id="Ashish Vaswani">
      <data key="d0">Ashish Vaswani</data>
      <data key="d1">person</data>
      <data key="d2">Ashish Vaswani is affiliated with Google Brain and contributed equally to the work, involved in designing and implementing the first Transformer models.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Noam Shazeer">
      <data key="d0">Noam Shazeer</data>
      <data key="d1">person</data>
      <data key="d2">Noam Shazeer is affiliated with Google Brain and contributed equally to the work, proposed scaled dot-product attention, multi-head attention, and the parameter-free position representation.&lt;SEP&gt;Noam Shazeer is an author of a paper on outrageously large neural networks.&lt;SEP&gt;Noam Shazeer is an author of papers on exploring the limits of language modeling and outrageously large neural networks.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Niki Parmar">
      <data key="d0">Niki Parmar</data>
      <data key="d1">person</data>
      <data key="d2">Niki Parmar is affiliated with Google Research and contributed equally to the work, designed, implemented, tuned, and evaluated countless model variants.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jakob Uszkoreit">
      <data key="d0">Jakob Uszkoreit</data>
      <data key="d1">person</data>
      <data key="d2">Jakob Uszkoreit is affiliated with Google Research and contributed equally to the work, proposed replacing RNNs with self-attention and started the effort to evaluate this idea.&lt;SEP&gt;Jakob Uszkoreit is an author of a paper on a decomposable attention model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Llion Jones">
      <data key="d0">Llion Jones</data>
      <data key="d1">person</data>
      <data key="d2">Llion Jones is affiliated with Google Research and contributed equally to the work, experimented with novel model variants, was responsible for the initial codebase, and efficient inference and visualizations.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Aidan N. Gomez">
      <data key="d0">Aidan N. Gomez</data>
      <data key="d1">person</data>
      <data key="d2">Aidan N. Gomez is affiliated with the University of Toronto and contributed equally to the work, spent countless long days designing various parts of and implementing tensor2tensor.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Łukasz Kaiser">
      <data key="d0">Łukasz Kaiser</data>
      <data key="d1">person</data>
      <data key="d2">Łukasz Kaiser is affiliated with Google Brain and contributed equally to the work, spent countless long days designing various parts of and implementing tensor2tensor, replacing the earlier codebase.&lt;SEP&gt;Łukasz Kaiser is an author of papers on neural GPUs learn algorithms and can active memory replace attention.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Illia Polosukhin">
      <data key="d0">Illia Polosukhin</data>
      <data key="d1">person</data>
      <data key="d2">Illia Polosukhin is affiliated and contributed equally to the work, designed and implemented the first Transformer models.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Google Brain">
      <data key="d0">Google Brain</data>
      <data key="d1">organization</data>
      <data key="d2">Google Brain is a research organization where some of the authors are affiliated, involved in the development of the Transformer model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Google Research">
      <data key="d0">Google Research</data>
      <data key="d1">organization</data>
      <data key="d2">Google Research is a research organization where some of the authors are affiliated, involved in the development of the Transformer model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="University of Toronto">
      <data key="d0">University of Toronto</data>
      <data key="d1">organization</data>
      <data key="d2">University of Toronto is an academic institution where Aidan N. Gomez is affiliated.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-to-German Translation Task">
      <data key="d0">WMT 2014 English-to-German Translation Task</data>
      <data key="d1">event</data>
      <data key="d2">WMT 2014 English-to-German translation task is a machine translation benchmark where the Transformer model achieved 28.4 BLEU.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-to-French Translation Task">
      <data key="d0">WMT 2014 English-to-French Translation Task</data>
      <data key="d1">event</data>
      <data key="d2">WMT 2014 English-to-French translation task is a machine translation benchmark where the Transformer model achieved a new single-model state-of-the-art BLEU score of 41.0.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Long Beach, CA, USA">
      <data key="d0">Long Beach, CA, USA</data>
      <data key="d1">geo</data>
      <data key="d2">Long Beach, CA, USA is the location of the 31st Conference on Neural Information Processing Systems (NIPS 2017).</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="31st Conference on Neural Information Processing Systems (NIPS 2017)">
      <data key="d0">31st Conference on Neural Information Processing Systems (NIPS 2017)</data>
      <data key="d1">event</data>
      <data key="d2">31st Conference on Neural Information Processing Systems (NIPS 2017) is a conference where the paper was presented.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Machine Translation">
      <data key="d0">Machine Translation</data>
      <data key="d1">category</data>
      <data key="d2">A category of natural language processing tasks that involve translating text from one language to another.&lt;SEP&gt;Machine Translation is a category of tasks that the Transformer model addresses.&lt;SEP&gt;Machine translation is the automated translation of text from one language to another.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Transformer">
      <data key="d0">Transformer</data>
      <data key="d1">category</data>
      <data key="d2">A novel network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions.&lt;SEP&gt;A transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.&lt;SEP&gt;The Transformer is a model that uses multi-head attention in three different ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder.&lt;SEP&gt;The Transformer is a sequence transduction model based entirely on attention mechanisms, replacing recurrent layers in encoder-decoder architectures.&lt;SEP&gt;Transformer is a new simple network architecture based solely on attention mechanisms.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59&lt;SEP&gt;chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Is All You Need">
      <data key="d0">Attention Is All You Need</data>
      <data key="d1">category</data>
      <data key="d2">Title of the research paper introducing the Transformer model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sequence Transduction Models">
      <data key="d0">Sequence Transduction Models</data>
      <data key="d1">category</data>
      <data key="d2">The dominant models for sequence transduction, which the Transformer aims to improve upon.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Recurrent Neural Networks">
      <data key="d0">Recurrent Neural Networks</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural network architecture used for sequence modeling.&lt;SEP&gt;A type of neural network that the Transformer replaces, known for sequential computation.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Convolutional Neural Networks">
      <data key="d0">Convolutional Neural Networks</data>
      <data key="d1">category</data>
      <data key="d2">Another type of neural network that the Transformer replaces, used in sequence modeling.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Encoder">
      <data key="d0">Encoder</data>
      <data key="d1">category</data>
      <data key="d2">A component of sequence transduction models that processes the input sequence.&lt;SEP&gt;A component of the Transformer model that maps an input sequence of symbol representations to a sequence of continuous representations.&lt;SEP&gt;The encoder is a component of the Transformer model that contains self-attention layers and feed-forward networks.&lt;SEP&gt;The encoder is a part of sequence transduction models, mapping an input sequence to a sequence of hidden representations.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Decoder">
      <data key="d0">Decoder</data>
      <data key="d1">category</data>
      <data key="d2">A component of sequence transduction models that generates the output sequence.&lt;SEP&gt;A component of the Transformer model that generates an output sequence of symbols one element at a time based on the encoder's output.&lt;SEP&gt;The decoder is a component of the Transformer model that contains self-attention layers, encoder-decoder attention layers, and feed-forward networks. It generates the output sequence.&lt;SEP&gt;The decoder is a part of sequence transduction models, mapping a sequence of hidden representations to an output sequence.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Mechanism">
      <data key="d0">Attention Mechanism</data>
      <data key="d1">category</data>
      <data key="d2">A mechanism used to connect the encoder and decoder, and the core component of the Transformer.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Machine Translation Tasks">
      <data key="d0">Machine Translation Tasks</data>
      <data key="d1">event</data>
      <data key="d2">Tasks used to evaluate the performance of the Transformer model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="BLEU">
      <data key="d0">BLEU</data>
      <data key="d1">category</data>
      <data key="d2">A metric used to evaluate the quality of machine translation output.&lt;SEP&gt;A metric used to evaluate the quality of machine translation.&lt;SEP&gt;BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of machine translation output.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4&lt;SEP&gt;chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="P100 GPUs">
      <data key="d0">P100 GPUs</data>
      <data key="d1">category</data>
      <data key="d2">Hardware used to train the Transformer model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Long Short-Term Memory">
      <data key="d0">Long Short-Term Memory</data>
      <data key="d1">category</data>
      <data key="d2">A type of recurrent neural network.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Gated Recurrent">
      <data key="d0">Gated Recurrent</data>
      <data key="d1">category</data>
      <data key="d2">A type of recurrent neural network.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sequence Modeling">
      <data key="d0">Sequence Modeling</data>
      <data key="d1">category</data>
      <data key="d2">A field that uses recurrent neural networks.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Transduction Problems">
      <data key="d0">Transduction Problems</data>
      <data key="d1">category</data>
      <data key="d2">A field that uses recurrent neural networks.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Language Modeling">
      <data key="d0">Language Modeling</data>
      <data key="d1">category</data>
      <data key="d2">A category of natural language processing tasks that involve predicting the probability of a sequence of words.&lt;SEP&gt;A type of sequence modeling.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Encoder-Decoder Architectures">
      <data key="d0">Encoder-Decoder Architectures</data>
      <data key="d1">category</data>
      <data key="d2">A type of architecture used in sequence modeling.&lt;SEP&gt;A type of neural network architecture commonly used for sequence transduction tasks.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="RNNs">
      <data key="d0">RNNs</data>
      <data key="d1">category</data>
      <data key="d2">Abbreviation for Recurrent Neural Networks.&lt;SEP&gt;Recurrent Neural Networks, a type of neural network architecture, specifically sequence-aligned RNNs, which the Transformer aims to replace with self-attention.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Self-Attention">
      <data key="d0">Self-Attention</data>
      <data key="d1">category</data>
      <data key="d2">An attention mechanism relating different positions of a single sequence to compute a representation of the sequence.&lt;SEP&gt;An attention mechanism relating different positions of a single sequence.&lt;SEP&gt;Self-Attention is a mechanism used for mapping variable-length sequences, connecting all positions with a constant number of sequentially executed operations.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Extended Neural GPU">
      <data key="d0">Extended Neural GPU</data>
      <data key="d1">category</data>
      <data key="d2">An architecture that uses convolutional neural networks.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="ByteNet">
      <data key="d0">ByteNet</data>
      <data key="d1">category</data>
      <data key="d2">An architecture that uses convolutional neural networks.&lt;SEP&gt;ByteNet is a neural machine translation model.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="ConvS2S">
      <data key="d0">ConvS2S</data>
      <data key="d1">category</data>
      <data key="d2">An architecture that uses convolutional neural networks.&lt;SEP&gt;ConvS2S is a convolutional sequence-to-sequence model used for machine translation.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Multi-Head Attention">
      <data key="d0">Multi-Head Attention</data>
      <data key="d1">category</data>
      <data key="d2">A type of attention used in the Transformer.&lt;SEP&gt;An attention mechanism used to counteract the effects of attention-weighted positions.&lt;SEP&gt;Multi-head attention is a mechanism that allows the model to jointly attend to information from different representation subspaces at different positions.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Reading Comprehension">
      <data key="d0">Reading Comprehension</data>
      <data key="d1">category</data>
      <data key="d2">A task that uses self-attention.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Abstractive Summarization">
      <data key="d0">Abstractive Summarization</data>
      <data key="d1">category</data>
      <data key="d2">A category of natural language processing tasks that involve generating summaries of text.&lt;SEP&gt;A task that uses self-attention.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Textual Entailment">
      <data key="d0">Textual Entailment</data>
      <data key="d1">category</data>
      <data key="d2">A task that uses self-attention.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Tensor2tensor">
      <data key="d0">Tensor2tensor</data>
      <data key="d1">category</data>
      <data key="d2">A codebase used to implement the Transformer.</data>
      <data key="d3">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Scaled Dot-Product Attention">
      <data key="d0">Scaled Dot-Product Attention</data>
      <data key="d1">category</data>
      <data key="d2">A specific type of attention mechanism used, involving the computation of dot products between queries and keys, scaled by the square root of the dimension of the keys.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Convolution">
      <data key="d0">Convolution</data>
      <data key="d1">category</data>
      <data key="d2">A mathematical operation used in signal processing and image analysis, and an alternative to self-attention that the Transformer avoids.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="End-To-End Memory Networks">
      <data key="d0">End-To-End Memory Networks</data>
      <data key="d1">organization</data>
      <data key="d2">A type of neural network architecture based on a recurrent attention mechanism, used for question answering and language modeling.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Additive Attention">
      <data key="d0">Additive Attention</data>
      <data key="d1">category</data>
      <data key="d2">An attention function using a feed-forward network to compute compatibility, an alternative to dot-product attention.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Dot-Product Attention">
      <data key="d0">Dot-Product Attention</data>
      <data key="d1">category</data>
      <data key="d2">An attention function that computes compatibility using dot products, similar to scaled dot-product attention but without the scaling factor.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Layer Normalization">
      <data key="d0">Layer Normalization</data>
      <data key="d1">category</data>
      <data key="d2">A technique for normalizing the activations of a neural network layer.&lt;SEP&gt;A technique used to normalize the outputs of each layer to improve training stability and speed.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Feed-Forward Network">
      <data key="d0">Feed-Forward Network</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural network with fully connected layers used in both additive attention and within the encoder and decoder stacks of the Transformer.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Q">
      <data key="d0">Q</data>
      <data key="d1">category</data>
      <data key="d2">Matrix of queries used in the Scaled Dot-Product Attention mechanism.&lt;SEP&gt;Q represents the queries in the attention mechanism.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="K">
      <data key="d0">K</data>
      <data key="d1">category</data>
      <data key="d2">K represents the keys in the attention mechanism.&lt;SEP&gt;Matrix of keys used in the Scaled Dot-Product Attention mechanism.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="V">
      <data key="d0">V</data>
      <data key="d1">category</data>
      <data key="d2">Matrix of values used in the Scaled Dot-Product Attention mechanism.&lt;SEP&gt;V represents the values in the attention mechanism.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Figure 1">
      <data key="d0">Figure 1</data>
      <data key="d1">category</data>
      <data key="d2">A diagram depicting the Transformer model architecture, including encoder and decoder stacks.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Figure 2">
      <data key="d0">Figure 2</data>
      <data key="d1">category</data>
      <data key="d2">A diagram illustrating Scaled Dot-Product Attention and Multi-Head Attention mechanisms.</data>
      <data key="d3">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="ReLU">
      <data key="d0">ReLU</data>
      <data key="d1">category</data>
      <data key="d2">ReLU is an activation function used in the feed-forward network of the encoder and decoder layers.</data>
      <data key="d3">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Softmax Function">
      <data key="d0">Softmax Function</data>
      <data key="d1">category</data>
      <data key="d2">The softmax function is used to convert the decoder output to predicted next-token probabilities.</data>
      <data key="d3">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Positional Encoding">
      <data key="d0">Positional Encoding</data>
      <data key="d1">category</data>
      <data key="d2">Positional encodings are added to the input embeddings to provide information about the position of tokens in the sequence.</data>
      <data key="d3">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-German Dataset">
      <data key="d0">WMT 2014 English-German Dataset</data>
      <data key="d1">category</data>
      <data key="d2">The WMT 2014 English-German dataset is a standard dataset consisting of about 4.5 million sentence pairs used for training models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-French Dataset">
      <data key="d0">WMT 2014 English-French Dataset</data>
      <data key="d1">category</data>
      <data key="d2">The WMT 2014 English-French dataset is a larger dataset consisting of 36M sentences used for training models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="NVIDIA P100 GPUs">
      <data key="d0">NVIDIA P100 GPUs</data>
      <data key="d1">organization</data>
      <data key="d2">NVIDIA P100 GPUs are hardware used for training the models, with the training done on one machine with 8 GPUs.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Positional Encodings">
      <data key="d0">Positional Encodings</data>
      <data key="d1">category</data>
      <data key="d2">Positional encodings are added to embeddings to provide information about the position of tokens in a sequence.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sine and Cosine Functions">
      <data key="d0">Sine and Cosine Functions</data>
      <data key="d1">category</data>
      <data key="d2">Sine and cosine functions of different frequencies are used as positional encodings.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Word-Piece">
      <data key="d0">Word-Piece</data>
      <data key="d1">category</data>
      <data key="d2">Word-piece representations are used in machine translation models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Byte-Pair">
      <data key="d0">Byte-Pair</data>
      <data key="d1">category</data>
      <data key="d2">Byte-pair representations are used in machine translation models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Convolutional Layers">
      <data key="d0">Convolutional Layers</data>
      <data key="d1">category</data>
      <data key="d2">Convolutional layers are used for mapping variable-length sequences and are compared to self-attention layers.&lt;SEP&gt;Layers in a neural network that process data using convolution operations.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Recurrent Layers">
      <data key="d0">Recurrent Layers</data>
      <data key="d1">category</data>
      <data key="d2">Layers in a neural network that process sequential data.&lt;SEP&gt;Recurrent layers are used for mapping variable-length sequences and are compared to self-attention layers.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014">
      <data key="d0">WMT 2014</data>
      <data key="d1">event</data>
      <data key="d2">WMT 2014 is a machine translation competition and dataset provider.&lt;SEP&gt;WMT 2014 is a machine translation competition used to evaluate the performance of translation models.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="English-German Dataset">
      <data key="d0">English-German Dataset</data>
      <data key="d1">category</data>
      <data key="d2">The English-German dataset is used for training machine translation models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="English-French Dataset">
      <data key="d0">English-French Dataset</data>
      <data key="d1">category</data>
      <data key="d2">The English-French dataset is used for training machine translation models.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="NVIDIA">
      <data key="d0">NVIDIA</data>
      <data key="d1">organization</data>
      <data key="d2">NVIDIA is a technology company known for designing and manufacturing graphics processing units (GPUs).&lt;SEP&gt;NVIDIA is a technology company that produces GPUs used for training machine learning models.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Geometric Progression">
      <data key="d0">Geometric Progression</data>
      <data key="d1">category</data>
      <data key="d2">Geometric progression is the pattern with which wavelengths form the positional encoding.</data>
      <data key="d3">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="P100">
      <data key="d0">P100</data>
      <data key="d1">category</data>
      <data key="d2">P100 GPUs are high-performance GPUs used for training machine learning models.&lt;SEP&gt;P100 is a type of GPU with a specified TFLOPS value of 9.5.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Adam">
      <data key="d0">Adam</data>
      <data key="d1">category</data>
      <data key="d2">Adam is an optimization algorithm used in training machine learning models.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Deep-Att + PosUnk">
      <data key="d0">Deep-Att + PosUnk</data>
      <data key="d1">category</data>
      <data key="d2">Deep-Att + PosUnk is a neural machine translation model that incorporates deep attention mechanisms and positional embeddings.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="GNMT + RL">
      <data key="d0">GNMT + RL</data>
      <data key="d1">organization</data>
      <data key="d2">GNMT + RL is Google's Neural Machine Translation system with Reinforcement Learning.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="MoE">
      <data key="d0">MoE</data>
      <data key="d1">organization</data>
      <data key="d2">MoE refers to Mixture of Experts, a model architecture used for machine translation.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Transformer (base model)">
      <data key="d0">Transformer (base model)</data>
      <data key="d1">category</data>
      <data key="d2">The Transformer (base model) is a baseline model architecture for machine translation, as described in the paper.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Transformer (big)">
      <data key="d0">Transformer (big)</data>
      <data key="d1">category</data>
      <data key="d2">The Transformer (big) is a larger version of the Transformer model, with increased capacity and performance.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="English-to-German Translation">
      <data key="d0">English-to-German Translation</data>
      <data key="d1">category</data>
      <data key="d2">English-to-German translation is a specific task in machine translation.&lt;SEP&gt;English-to-German translation is a task used to evaluate the importance of different components of the Transformer model on the development set newstest2013.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="English-to-French Translation">
      <data key="d0">English-to-French Translation</data>
      <data key="d1">category</data>
      <data key="d2">English-to-French translation is a specific task in machine translation.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Source Tokens">
      <data key="d0">Source Tokens</data>
      <data key="d1">category</data>
      <data key="d2">Source tokens represent the input sequence in a machine translation task.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Target Tokens">
      <data key="d0">Target Tokens</data>
      <data key="d1">category</data>
      <data key="d2">Target tokens represent the output sequence in a machine translation task.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Base Models">
      <data key="d0">Base Models</data>
      <data key="d1">category</data>
      <data key="d2">The base models are the standard Transformer models used for comparison.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Big Models">
      <data key="d0">Big Models</data>
      <data key="d1">category</data>
      <data key="d2">The big models are larger Transformer models with more parameters, used for achieving higher accuracy.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Warmup Steps">
      <data key="d0">Warmup Steps</data>
      <data key="d1">category</data>
      <data key="d2">Warmup steps refer to the initial phase of training where the learning rate is gradually increased.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Residual Dropout">
      <data key="d0">Residual Dropout</data>
      <data key="d1">category</data>
      <data key="d2">Residual dropout is a regularization technique applied to the output of each sub-layer in the Transformer model.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Label Smoothing">
      <data key="d0">Label Smoothing</data>
      <data key="d1">category</data>
      <data key="d2">Label smoothing is a regularization technique that improves accuracy and BLEU score by making the model more unsure.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Newstest2014">
      <data key="d0">Newstest2014</data>
      <data key="d1">event</data>
      <data key="d2">Newstest2014 is a specific evaluation dataset used in the WMT 2014 competition.</data>
      <data key="d3">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Newstest2013">
      <data key="d0">Newstest2013</data>
      <data key="d1">event</data>
      <data key="d2">Newstest2013 is a development dataset used to evaluate the performance of models.&lt;SEP&gt;Newstest2013 is a development set used to measure the change in performance of the Transformer model on English-to-German translation.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="GPUs">
      <data key="d0">GPUs</data>
      <data key="d1">category</data>
      <data key="d2">GPUs (Graphics Processing Units) are used to train the model by multiplying the training time and number of GPUs used by an estimate of the sustained single-precision floating-point capacity of each GPU.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Beam Search">
      <data key="d0">Beam Search</data>
      <data key="d1">category</data>
      <data key="d2">Beam search is a search algorithm used in the evaluation of the Transformer model for English-to-German translation.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Heads">
      <data key="d0">Attention Heads</data>
      <data key="d1">category</data>
      <data key="d2">Attention heads are components of the Transformer architecture that are varied to evaluate their impact on model performance.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="K80">
      <data key="d0">K80</data>
      <data key="d1">category</data>
      <data key="d2">K80 is a type of GPU with a specified TFLOPS value of 2.8.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="K40">
      <data key="d0">K40</data>
      <data key="d1">category</data>
      <data key="d2">K40 is a type of GPU with a specified TFLOPS value of 3.7.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="M40">
      <data key="d0">M40</data>
      <data key="d1">category</data>
      <data key="d2">M40 is a type of GPU with a specified TFLOPS value of 6.0.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-to-German">
      <data key="d0">WMT 2014 English-to-German</data>
      <data key="d1">event</data>
      <data key="d2">WMT 2014 English-to-German is a translation task where the Transformer achieved a new state of the art.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="WMT 2014 English-to-French">
      <data key="d0">WMT 2014 English-to-French</data>
      <data key="d1">event</data>
      <data key="d2">WMT 2014 English-to-French is a translation task where the Transformer achieved a new state of the art.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Nal Kalchbrenner">
      <data key="d0">Nal Kalchbrenner</data>
      <data key="d1">person</data>
      <data key="d2">Nal Kalchbrenner is acknowledged for providing fruitful comments, corrections, and inspiration.&lt;SEP&gt;Nal Kalchbrenner is an author of a paper on neural machine translation in linear time.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Stephan Gouws">
      <data key="d0">Stephan Gouws</data>
      <data key="d1">person</data>
      <data key="d2">Stephan Gouws is acknowledged for providing fruitful comments, corrections, and inspiration.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="TensorFlow">
      <data key="d0">TensorFlow</data>
      <data key="d1">organization</data>
      <data key="d2">TensorFlow is the organization that hosts the code used to train and evaluate the models at https://github.com/tensorflow/tensor2tensor.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Floating Point Operations">
      <data key="d0">Floating Point Operations</data>
      <data key="d1">category</data>
      <data key="d2">Mathematical operations used to train a model, influencing training time and resource allocation.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Single-Precision Floating-Point Capacity">
      <data key="d0">Single-Precision Floating-Point Capacity</data>
      <data key="d1">category</data>
      <data key="d2">A measure of a GPU's ability to perform floating-point operations, impacting the speed of model training.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Model Variations">
      <data key="d0">Model Variations</data>
      <data key="d1">category</data>
      <data key="d2">Different configurations of the Transformer model tested to evaluate the importance of individual components.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Development Set">
      <data key="d0">Development Set</data>
      <data key="d1">category</data>
      <data key="d2">A dataset used to evaluate the performance of different model variations.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Checkpoint Averaging">
      <data key="d0">Checkpoint Averaging</data>
      <data key="d1">category</data>
      <data key="d2">A technique used to improve model performance, not used in this specific evaluation.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Table 3">
      <data key="d0">Table 3</data>
      <data key="d1">category</data>
      <data key="d2">A table presenting the results of varying the Transformer architecture.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Key">
      <data key="d0">Attention Key</data>
      <data key="d1">category</data>
      <data key="d2">A component of the attention mechanism in the Transformer model.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Value Dimensions">
      <data key="d0">Attention Value Dimensions</data>
      <data key="d1">category</data>
      <data key="d2">A component of the attention mechanism in the Transformer model.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Single-Head Attention">
      <data key="d0">Single-Head Attention</data>
      <data key="d1">category</data>
      <data key="d2">An attention mechanism with a single attention head.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Perplexity">
      <data key="d0">Perplexity</data>
      <data key="d1">category</data>
      <data key="d2">A measure of how well a probability distribution predicts a sample.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Byte-Pair Encoding">
      <data key="d0">Byte-Pair Encoding</data>
      <data key="d1">category</data>
      <data key="d2">A method of encoding text into word pieces.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Base Model">
      <data key="d0">Base Model</data>
      <data key="d1">category</data>
      <data key="d2">The standard Transformer model used as a baseline for comparison.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rows (A)">
      <data key="d0">Rows (A)</data>
      <data key="d1">category</data>
      <data key="d2">Refers to the section of Table 3 that shows the variations in the number of attention heads and the attention key and value dimensions.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rows (B)">
      <data key="d0">Rows (B)</data>
      <data key="d1">category</data>
      <data key="d2">Refers to the section of Table 3 that shows the variations in attention key size.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rows (C)">
      <data key="d0">Rows (C)</data>
      <data key="d1">category</data>
      <data key="d2">Refers to the section of Table 3 that shows the variations in model size.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rows (D)">
      <data key="d0">Rows (D)</data>
      <data key="d1">category</data>
      <data key="d2">Refers to the section of Table 3 that shows the variations in dropout rate.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Dropout">
      <data key="d0">Dropout</data>
      <data key="d1">category</data>
      <data key="d2">A regularization technique used to prevent overfitting.&lt;SEP&gt;Dropout is a regularization technique used to prevent overfitting in neural networks, as described in the paper by Srivastava et al.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rows (E)">
      <data key="d0">Rows (E)</data>
      <data key="d1">category</data>
      <data key="d2">Refers to the section of Table 3 that shows the variations in positional encoding.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sinusoidal Positional Encoding">
      <data key="d0">Sinusoidal Positional Encoding</data>
      <data key="d1">category</data>
      <data key="d2">A method of encoding the position of words in a sequence.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Learned Positional Embeddings">
      <data key="d0">Learned Positional Embeddings</data>
      <data key="d1">category</data>
      <data key="d2">A method of encoding the position of words in a sequence.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Multi-Headed Self-Attention">
      <data key="d0">Multi-Headed Self-Attention</data>
      <data key="d1">category</data>
      <data key="d2">An attention mechanism that uses multiple attention heads.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Translation Tasks">
      <data key="d0">Translation Tasks</data>
      <data key="d1">category</data>
      <data key="d2">Tasks involving the translation of text from one language to another.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Ensembles">
      <data key="d0">Ensembles</data>
      <data key="d1">category</data>
      <data key="d2">A combination of multiple models to improve performance.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Local, Restricted Attention Mechanisms">
      <data key="d0">Local, Restricted Attention Mechanisms</data>
      <data key="d1">category</data>
      <data key="d2">Attention mechanisms that only attend to a local region of the input.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Images, Audio and Video">
      <data key="d0">Images, Audio and Video</data>
      <data key="d1">category</data>
      <data key="d2">Examples of input and output modalities that the Transformer could be extended to.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jimmy Lei Ba">
      <data key="d0">Jimmy Lei Ba</data>
      <data key="d1">person</data>
      <data key="d2">Jimmy Lei Ba is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jamie Ryan Kiros">
      <data key="d0">Jamie Ryan Kiros</data>
      <data key="d1">person</data>
      <data key="d2">Jamie Ryan Kiros is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Geoffrey E Hinton">
      <data key="d0">Geoffrey E Hinton</data>
      <data key="d1">person</data>
      <data key="d2">Geoffrey E Hinton is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Neural Machine Translation">
      <data key="d0">Neural Machine Translation</data>
      <data key="d1">category</data>
      <data key="d2">A technique for automatically translating text from one language to another using neural networks.&lt;SEP&gt;Neural Machine Translation is a type of machine translation that uses neural networks.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Dzmitry Bahdanau">
      <data key="d0">Dzmitry Bahdanau</data>
      <data key="d1">person</data>
      <data key="d2">Dzmitry Bahdanau is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Kyunghyun Cho">
      <data key="d0">Kyunghyun Cho</data>
      <data key="d1">person</data>
      <data key="d2">Kyunghyun Cho is an author of papers on phrase representations using rnn encoder-decoder and gated recurrent neural networks.&lt;SEP&gt;Kyunghyun Cho is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Yoshua Bengio">
      <data key="d0">Yoshua Bengio</data>
      <data key="d1">person</data>
      <data key="d2">Yoshua Bengio is an author of papers on phrase representations using rnn encoder-decoder, gated recurrent neural networks, and a structured self-attentive sentence embedding.&lt;SEP&gt;Yoshua Bengio is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Denny Britz">
      <data key="d0">Denny Britz</data>
      <data key="d1">person</data>
      <data key="d2">Denny Britz is an author of a paper on neural machine translation architectures.&lt;SEP&gt;Denny Britz is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Anna Goldie">
      <data key="d0">Anna Goldie</data>
      <data key="d1">person</data>
      <data key="d2">Anna Goldie is an author of a paper on neural machine translation architectures.&lt;SEP&gt;Anna Goldie is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Minh-Thang Luong">
      <data key="d0">Minh-Thang Luong</data>
      <data key="d1">person</data>
      <data key="d2">Minh-Thang Luong is an author of papers on neural machine translation architectures and attention-based neural machine translation.&lt;SEP&gt;Minh-Thang Luong is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Quoc V. Le">
      <data key="d0">Quoc V. Le</data>
      <data key="d1">person</data>
      <data key="d2">Quoc V. Le is an author of a paper on neural machine translation architectures and outrageously large neural networks.&lt;SEP&gt;Quoc V. Le is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jianpeng Cheng">
      <data key="d0">Jianpeng Cheng</data>
      <data key="d1">person</data>
      <data key="d2">Jianpeng Cheng is an author of a paper on long short-term memory-networks for machine reading.&lt;SEP&gt;Jianpeng Cheng is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Li Dong">
      <data key="d0">Li Dong</data>
      <data key="d1">person</data>
      <data key="d2">Li Dong is an author of a paper on long short-term memory-networks for machine reading.&lt;SEP&gt;Li Dong is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Mirella Lapata">
      <data key="d0">Mirella Lapata</data>
      <data key="d1">person</data>
      <data key="d2">Mirella Lapata is an author of a paper on long short-term memory-networks for machine reading.&lt;SEP&gt;Mirella Lapata is an author referenced in the paper.</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Bart van Merrienboer">
      <data key="d0">Bart van Merrienboer</data>
      <data key="d1">person</data>
      <data key="d2">Bart van Merrienboer is an author of a paper on phrase representations using rnn encoder-decoder.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Caglar Gulcehre">
      <data key="d0">Caglar Gulcehre</data>
      <data key="d1">person</data>
      <data key="d2">Caglar Gulcehre is an author of papers on phrase representations using rnn encoder-decoder and gated recurrent neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Fethi Bougares">
      <data key="d0">Fethi Bougares</data>
      <data key="d1">person</data>
      <data key="d2">Fethi Bougares is an author of a paper on phrase representations using rnn encoder-decoder.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Holger Schwenk">
      <data key="d0">Holger Schwenk</data>
      <data key="d1">person</data>
      <data key="d2">Holger Schwenk is an author of a paper on phrase representations using rnn encoder-decoder.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Francois Chollet">
      <data key="d0">Francois Chollet</data>
      <data key="d1">person</data>
      <data key="d2">Francois Chollet is the author of a paper on Xception: Deep learning with depthwise separable convolutions.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Junyoung Chung">
      <data key="d0">Junyoung Chung</data>
      <data key="d1">person</data>
      <data key="d2">Junyoung Chung is an author of a paper on gated recurrent neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jonas Gehring">
      <data key="d0">Jonas Gehring</data>
      <data key="d1">person</data>
      <data key="d2">Jonas Gehring is an author of a paper on convolutional sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Michael Auli">
      <data key="d0">Michael Auli</data>
      <data key="d1">person</data>
      <data key="d2">Michael Auli is an author of a paper on convolutional sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="David Grangier">
      <data key="d0">David Grangier</data>
      <data key="d1">person</data>
      <data key="d2">David Grangier is an author of a paper on convolutional sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Denis Yarats">
      <data key="d0">Denis Yarats</data>
      <data key="d1">person</data>
      <data key="d2">Denis Yarats is an author of a paper on convolutional sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Yann N. Dauphin">
      <data key="d0">Yann N. Dauphin</data>
      <data key="d1">person</data>
      <data key="d2">Yann N. Dauphin is an author of a paper on convolutional sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Alex Graves">
      <data key="d0">Alex Graves</data>
      <data key="d1">person</data>
      <data key="d2">Alex Graves is an author of papers on generating sequences with recurrent neural networks and neural machine translation in linear time.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Kaiming He">
      <data key="d0">Kaiming He</data>
      <data key="d1">person</data>
      <data key="d2">Kaiming He is an author of a paper on deep residual learning for image recognition.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Xiangyu Zhang">
      <data key="d0">Xiangyu Zhang</data>
      <data key="d1">person</data>
      <data key="d2">Xiangyu Zhang is an author of a paper on deep residual learning for image recognition.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Shaoqing Ren">
      <data key="d0">Shaoqing Ren</data>
      <data key="d1">person</data>
      <data key="d2">Shaoqing Ren is an author of a paper on deep residual learning for image recognition.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jian Sun">
      <data key="d0">Jian Sun</data>
      <data key="d1">person</data>
      <data key="d2">Jian Sun is an author of a paper on deep residual learning for image recognition.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sepp Hochreiter">
      <data key="d0">Sepp Hochreiter</data>
      <data key="d1">person</data>
      <data key="d2">Sepp Hochreiter is an author of papers on gradient flow in recurrent nets and long short-term memory.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Paolo Frasconi">
      <data key="d0">Paolo Frasconi</data>
      <data key="d1">person</data>
      <data key="d2">Paolo Frasconi is an author of a paper on gradient flow in recurrent nets.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jürgen Schmidhuber">
      <data key="d0">Jürgen Schmidhuber</data>
      <data key="d1">person</data>
      <data key="d2">Jürgen Schmidhuber is an author of papers on gradient flow in recurrent nets and long short-term memory.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rafal Jozefowicz">
      <data key="d0">Rafal Jozefowicz</data>
      <data key="d1">person</data>
      <data key="d2">Rafal Jozefowicz is an author of a paper on exploring the limits of language modeling.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Oriol Vinyals">
      <data key="d0">Oriol Vinyals</data>
      <data key="d1">person</data>
      <data key="d2">Oriol Vinyals is an author of a paper on exploring the limits of language modeling.&lt;SEP&gt;Oriol Vinyals is an author of a paper on sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Mike Schuster">
      <data key="d0">Mike Schuster</data>
      <data key="d1">person</data>
      <data key="d2">Mike Schuster is an author of a paper on Google's neural machine translation system.&lt;SEP&gt;Mike Schuster is an author of a paper on exploring the limits of language modeling.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Yonghui Wu">
      <data key="d0">Yonghui Wu</data>
      <data key="d1">person</data>
      <data key="d2">Yonghui Wu is an author of a paper on Google's neural machine translation system.&lt;SEP&gt;Yonghui Wu is an author of a paper on exploring the limits of language modeling.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Ilya Sutskever">
      <data key="d0">Ilya Sutskever</data>
      <data key="d1">person</data>
      <data key="d2">Ilya Sutskever is an author of a paper on neural GPUs learn algorithms.&lt;SEP&gt;Ilya Sutskever is an author of papers on dropout and sequence to sequence learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Lasse Espeholt">
      <data key="d0">Lasse Espeholt</data>
      <data key="d1">person</data>
      <data key="d2">Lasse Espeholt is an author of a paper on neural machine translation in linear time.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Karen Simonyan">
      <data key="d0">Karen Simonyan</data>
      <data key="d1">person</data>
      <data key="d2">Karen Simonyan is an author of a paper on neural machine translation in linear time.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Aaron van den Oord">
      <data key="d0">Aaron van den Oord</data>
      <data key="d1">person</data>
      <data key="d2">Aaron van den Oord is an author of a paper on neural machine translation in linear time.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Koray Kavukcuoglu">
      <data key="d0">Koray Kavukcuoglu</data>
      <data key="d1">person</data>
      <data key="d2">Koray Kavukcuoglu is an author of a paper on neural machine translation in linear time.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Yoon Kim">
      <data key="d0">Yoon Kim</data>
      <data key="d1">person</data>
      <data key="d2">Yoon Kim is an author of a paper on structured attention networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Carl Denton">
      <data key="d0">Carl Denton</data>
      <data key="d1">person</data>
      <data key="d2">Carl Denton is an author of a paper on structured attention networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Luong Hoang">
      <data key="d0">Luong Hoang</data>
      <data key="d1">person</data>
      <data key="d2">Luong Hoang is an author of a paper on structured attention networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Alexander M. Rush">
      <data key="d0">Alexander M. Rush</data>
      <data key="d1">person</data>
      <data key="d2">Alexander M. Rush is an author of a paper on structured attention networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Diederik Kingma">
      <data key="d0">Diederik Kingma</data>
      <data key="d1">person</data>
      <data key="d2">Diederik Kingma is an author of a paper on Adam: A method for stochastic optimization.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jimmy Ba">
      <data key="d0">Jimmy Ba</data>
      <data key="d1">person</data>
      <data key="d2">Jimmy Ba is an author of a paper on Adam: A method for stochastic optimization.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Oleksii Kuchaiev">
      <data key="d0">Oleksii Kuchaiev</data>
      <data key="d1">person</data>
      <data key="d2">Oleksii Kuchaiev is an author of a paper on Factorization tricks for LSTM networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Boris Ginsburg">
      <data key="d0">Boris Ginsburg</data>
      <data key="d1">person</data>
      <data key="d2">Boris Ginsburg is an author of a paper on Factorization tricks for LSTM networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Zhouhan Lin">
      <data key="d0">Zhouhan Lin</data>
      <data key="d1">person</data>
      <data key="d2">Zhouhan Lin is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Minwei Feng">
      <data key="d0">Minwei Feng</data>
      <data key="d1">person</data>
      <data key="d2">Minwei Feng is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Cicero Nogueira dos Santos">
      <data key="d0">Cicero Nogueira dos Santos</data>
      <data key="d1">person</data>
      <data key="d2">Cicero Nogueira dos Santos is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Mo Yu">
      <data key="d0">Mo Yu</data>
      <data key="d1">person</data>
      <data key="d2">Mo Yu is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Bing Xiang">
      <data key="d0">Bing Xiang</data>
      <data key="d1">person</data>
      <data key="d2">Bing Xiang is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Bowen Zhou">
      <data key="d0">Bowen Zhou</data>
      <data key="d1">person</data>
      <data key="d2">Bowen Zhou is an author of a paper on a structured self-attentive sentence embedding.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Samy Bengio">
      <data key="d0">Samy Bengio</data>
      <data key="d1">person</data>
      <data key="d2">Samy Bengio is an author of a paper on can active memory replace attention.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Hieu Pham">
      <data key="d0">Hieu Pham</data>
      <data key="d1">person</data>
      <data key="d2">Hieu Pham is an author of a paper on effective approaches to attention-based neural machine translation.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Christopher D Manning">
      <data key="d0">Christopher D Manning</data>
      <data key="d1">person</data>
      <data key="d2">Christopher D Manning is an author of a paper on effective approaches to attention-based neural machine translation.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Ankur Parikh">
      <data key="d0">Ankur Parikh</data>
      <data key="d1">person</data>
      <data key="d2">Ankur Parikh is an author of a paper on a decomposable attention model.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Oscar Täckström">
      <data key="d0">Oscar Täckström</data>
      <data key="d1">person</data>
      <data key="d2">Oscar Täckström is an author of a paper on a decomposable attention model.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Dipanjan Das">
      <data key="d0">Dipanjan Das</data>
      <data key="d1">person</data>
      <data key="d2">Dipanjan Das is an author of a paper on a decomposable attention model.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Romain Paulus">
      <data key="d0">Romain Paulus</data>
      <data key="d1">person</data>
      <data key="d2">Romain Paulus is an author of a paper on a deep reinforced model for abstractive summarization.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Caiming Xiong">
      <data key="d0">Caiming Xiong</data>
      <data key="d1">person</data>
      <data key="d2">Caiming Xiong is an author of a paper on a deep reinforced model for abstractive summarization.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Richard Socher">
      <data key="d0">Richard Socher</data>
      <data key="d1">person</data>
      <data key="d2">Richard Socher is an author of a paper on a deep reinforced model for abstractive summarization.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Oﬁr Press">
      <data key="d0">Oﬁr Press</data>
      <data key="d1">person</data>
      <data key="d2">Oﬁr Press is an author of a paper on using the output embedding to improve language models.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Lior Wolf">
      <data key="d0">Lior Wolf</data>
      <data key="d1">person</data>
      <data key="d2">Lior Wolf is an author of a paper on using the output embedding to improve language models.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rico Sennrich">
      <data key="d0">Rico Sennrich</data>
      <data key="d1">person</data>
      <data key="d2">Rico Sennrich is an author of a paper on neural machine translation of rare words with subword units.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Barry Haddow">
      <data key="d0">Barry Haddow</data>
      <data key="d1">person</data>
      <data key="d2">Barry Haddow is an author of a paper on neural machine translation of rare words with subword units.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Alexandra Birch">
      <data key="d0">Alexandra Birch</data>
      <data key="d1">person</data>
      <data key="d2">Alexandra Birch is an author of a paper on neural machine translation of rare words with subword units.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Azalia Mirhoseini">
      <data key="d0">Azalia Mirhoseini</data>
      <data key="d1">person</data>
      <data key="d2">Azalia Mirhoseini is an author of a paper on outrageously large neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Krzysztof Maziarz">
      <data key="d0">Krzysztof Maziarz</data>
      <data key="d1">person</data>
      <data key="d2">Krzysztof Maziarz is an author of a paper on outrageously large neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Andy Davis">
      <data key="d0">Andy Davis</data>
      <data key="d1">person</data>
      <data key="d2">Andy Davis is an author of a paper on outrageously large neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Geoffrey Hinton">
      <data key="d0">Geoffrey Hinton</data>
      <data key="d1">person</data>
      <data key="d2">Geoffrey Hinton is an author of a paper on outrageously large neural networks.&lt;SEP&gt;Geoffrey Hinton is an author of multiple papers on neural networks and machine learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jeff Dean">
      <data key="d0">Jeff Dean</data>
      <data key="d1">person</data>
      <data key="d2">Jeff Dean is an author of a paper on outrageously large neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="CoRR">
      <data key="d0">CoRR</data>
      <data key="d1">organization</data>
      <data key="d2">CoRR is a repository for electronic preprints of scientific papers, particularly in computer science.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="arXiv">
      <data key="d0">arXiv</data>
      <data key="d1">organization</data>
      <data key="d2">arXiv is a repository of electronic preprints for scientific papers.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="IEEE">
      <data key="d0">IEEE</data>
      <data key="d1">organization</data>
      <data key="d2">IEEE is a professional organization for electrical and electronics engineers.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="ICLR">
      <data key="d0">ICLR</data>
      <data key="d1">event</data>
      <data key="d2">International Conference on Learning Representations</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="NIPS">
      <data key="d0">NIPS</data>
      <data key="d1">event</data>
      <data key="d2">Advances in Neural Information Processing Systems</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Neural Machine Translation Architectures">
      <data key="d0">Neural Machine Translation Architectures</data>
      <data key="d1">category</data>
      <data key="d2">A category of neural network architectures designed for machine translation tasks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Long Short-Term Memory Networks">
      <data key="d0">Long Short-Term Memory Networks</data>
      <data key="d1">category</data>
      <data key="d2">A type of recurrent neural network architecture used for machine reading.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="RNN Encoder-Decoder">
      <data key="d0">RNN Encoder-Decoder</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural network architecture used for statistical machine translation.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Statistical Machine Translation">
      <data key="d0">Statistical Machine Translation</data>
      <data key="d1">category</data>
      <data key="d2">A category of machine translation methods that use statistical models.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Deep Learning">
      <data key="d0">Deep Learning</data>
      <data key="d1">category</data>
      <data key="d2">A category of machine learning algorithms based on artificial neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Depthwise Separable Convolutions">
      <data key="d0">Depthwise Separable Convolutions</data>
      <data key="d1">category</data>
      <data key="d2">A type of convolution used in deep learning.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Gated Recurrent Neural Networks">
      <data key="d0">Gated Recurrent Neural Networks</data>
      <data key="d1">category</data>
      <data key="d2">A type of recurrent neural network architecture used for sequence modeling.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sequence to Sequence Learning">
      <data key="d0">Sequence to Sequence Learning</data>
      <data key="d1">category</data>
      <data key="d2">A category of machine learning algorithms used for sequence prediction tasks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Image Recognition">
      <data key="d0">Image Recognition</data>
      <data key="d1">category</data>
      <data key="d2">A category of computer vision tasks that involve identifying objects in images.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Neural Computation">
      <data key="d0">Neural Computation</data>
      <data key="d1">category</data>
      <data key="d2">A field of study that combines neuroscience and computer science.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Neural GPUs">
      <data key="d0">Neural GPUs</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural network architecture designed to learn algorithms.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Linear Time">
      <data key="d0">Linear Time</data>
      <data key="d1">category</data>
      <data key="d2">A category of algorithms that have a time complexity of O(n).</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention Networks">
      <data key="d0">Attention Networks</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural network architecture that uses attention mechanisms.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Stochastic Optimization">
      <data key="d0">Stochastic Optimization</data>
      <data key="d1">category</data>
      <data key="d2">A category of optimization algorithms that use randomness.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="LSTM Networks">
      <data key="d0">LSTM Networks</data>
      <data key="d1">category</data>
      <data key="d2">A type of recurrent neural network architecture that uses long short-term memory cells.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sentence Embedding">
      <data key="d0">Sentence Embedding</data>
      <data key="d1">category</data>
      <data key="d2">A category of natural language processing tasks that involve mapping sentences to vectors.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Active Memory">
      <data key="d0">Active Memory</data>
      <data key="d1">category</data>
      <data key="d2">A type of memory used in neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Attention-Based Neural Machine Translation">
      <data key="d0">Attention-Based Neural Machine Translation</data>
      <data key="d1">category</data>
      <data key="d2">A type of neural machine translation that uses attention mechanisms.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Natural Language Processing">
      <data key="d0">Natural Language Processing</data>
      <data key="d1">category</data>
      <data key="d2">A field of study that combines computer science and linguistics.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Language Models">
      <data key="d0">Language Models</data>
      <data key="d1">category</data>
      <data key="d2">A category of natural language processing tasks that involve predicting the probability of a sequence of words.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Subword Units">
      <data key="d0">Subword Units</data>
      <data key="d1">category</data>
      <data key="d2">A type of unit used in neural machine translation.&lt;SEP&gt;Subword units are a technique used in natural language processing, as described in the referenced paper.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d0">Sparsely-Gated Mixture-of-Experts Layer</data>
      <data key="d1">category</data>
      <data key="d2">A sparsely-gated mixture-of-experts layer is a specific type of neural network architecture discussed in the paper by Shazeer et al.&lt;SEP&gt;A type of layer used in neural networks.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28&lt;SEP&gt;chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Gradient Flow">
      <data key="d0">Gradient Flow</data>
      <data key="d1">category</data>
      <data key="d2">A concept in deep learning that describes how gradients flow through a neural network.</data>
      <data key="d3">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Quoc Le">
      <data key="d0">Quoc Le</data>
      <data key="d1">person</data>
      <data key="d2">Quoc Le is an author of multiple papers on neural networks and machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Nitish Srivastava">
      <data key="d0">Nitish Srivastava</data>
      <data key="d1">person</data>
      <data key="d2">Nitish Srivastava is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Alex Krizhevsky">
      <data key="d0">Alex Krizhevsky</data>
      <data key="d1">person</data>
      <data key="d2">Alex Krizhevsky is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Ruslan Salakhutdinov">
      <data key="d0">Ruslan Salakhutdinov</data>
      <data key="d1">person</data>
      <data key="d2">Ruslan Salakhutdinov is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sainbayar Sukhbaatar">
      <data key="d0">Sainbayar Sukhbaatar</data>
      <data key="d1">person</data>
      <data key="d2">Sainbayar Sukhbaatar is an author of a paper on end-to-end memory networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Arthur Szlam">
      <data key="d0">Arthur Szlam</data>
      <data key="d1">person</data>
      <data key="d2">Arthur Szlam is an author of a paper on end-to-end memory networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jason Weston">
      <data key="d0">Jason Weston</data>
      <data key="d1">person</data>
      <data key="d2">Jason Weston is an author of a paper on end-to-end memory networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rob Fergus">
      <data key="d0">Rob Fergus</data>
      <data key="d1">person</data>
      <data key="d2">Rob Fergus is an author of a paper on end-to-end memory networks.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Christian Szegedy">
      <data key="d0">Christian Szegedy</data>
      <data key="d1">person</data>
      <data key="d2">Christian Szegedy is an author of a paper on the inception architecture for computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Vincent Vanhoucke">
      <data key="d0">Vincent Vanhoucke</data>
      <data key="d1">person</data>
      <data key="d2">Vincent Vanhoucke is an author of a paper on the inception architecture for computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sergey Ioffe">
      <data key="d0">Sergey Ioffe</data>
      <data key="d1">person</data>
      <data key="d2">Sergey Ioffe is an author of a paper on the inception architecture for computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jonathon Shlens">
      <data key="d0">Jonathon Shlens</data>
      <data key="d1">person</data>
      <data key="d2">Jonathon Shlens is an author of a paper on the inception architecture for computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Zbigniew Wojna">
      <data key="d0">Zbigniew Wojna</data>
      <data key="d1">person</data>
      <data key="d2">Zbigniew Wojna is an author of a paper on the inception architecture for computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Zhifeng Chen">
      <data key="d0">Zhifeng Chen</data>
      <data key="d1">person</data>
      <data key="d2">Zhifeng Chen is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Mohammad Norouzi">
      <data key="d0">Mohammad Norouzi</data>
      <data key="d1">person</data>
      <data key="d2">Mohammad Norouzi is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Wolfgang Macherey">
      <data key="d0">Wolfgang Macherey</data>
      <data key="d1">person</data>
      <data key="d2">Wolfgang Macherey is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Maxim Krikun">
      <data key="d0">Maxim Krikun</data>
      <data key="d1">person</data>
      <data key="d2">Maxim Krikun is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Yuan Cao">
      <data key="d0">Yuan Cao</data>
      <data key="d1">person</data>
      <data key="d2">Yuan Cao is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Qin Gao">
      <data key="d0">Qin Gao</data>
      <data key="d1">person</data>
      <data key="d2">Qin Gao is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Klaus Macherey">
      <data key="d0">Klaus Macherey</data>
      <data key="d1">person</data>
      <data key="d2">Klaus Macherey is an author of a paper on Google's neural machine translation system.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Jie Zhou">
      <data key="d0">Jie Zhou</data>
      <data key="d1">person</data>
      <data key="d2">Jie Zhou is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Ying Cao">
      <data key="d0">Ying Cao</data>
      <data key="d1">person</data>
      <data key="d2">Ying Cao is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Xuguang Wang">
      <data key="d0">Xuguang Wang</data>
      <data key="d1">person</data>
      <data key="d2">Xuguang Wang is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Peng Li">
      <data key="d0">Peng Li</data>
      <data key="d1">person</data>
      <data key="d2">Peng Li is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Wei Xu">
      <data key="d0">Wei Xu</data>
      <data key="d1">person</data>
      <data key="d2">Wei Xu is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Google">
      <data key="d0">Google</data>
      <data key="d1">organization</data>
      <data key="d2">Google is the organization behind the neural machine translation system described in one of the papers.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="ArXiv">
      <data key="d0">ArXiv</data>
      <data key="d1">organization</data>
      <data key="d2">ArXiv is a repository for preprints of scientific papers.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Outrageously Large Neural Networks">
      <data key="d0">Outrageously Large Neural Networks</data>
      <data key="d1">category</data>
      <data key="d2">Outrageously Large Neural Networks is the topic of a research paper by Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean, focusing on sparsely-gated mixture-of-experts layers.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Journal of Machine Learning Research">
      <data key="d0">Journal of Machine Learning Research</data>
      <data key="d1">organization</data>
      <data key="d2">The Journal of Machine Learning Research is a publication venue for research in machine learning.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="End-to-End Memory Networks">
      <data key="d0">End-to-End Memory Networks</data>
      <data key="d1">category</data>
      <data key="d2">End-to-end memory networks are a type of neural network architecture discussed in the paper by Sukhbaatar, Szlam, Weston, and Fergus.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Advances in Neural Information Processing Systems">
      <data key="d0">Advances in Neural Information Processing Systems</data>
      <data key="d1">event</data>
      <data key="d2">Advances in Neural Information Processing Systems (NeurIPS) is a prominent machine learning conference where research papers are presented.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Curran Associates, Inc.">
      <data key="d0">Curran Associates, Inc.</data>
      <data key="d1">organization</data>
      <data key="d2">Curran Associates, Inc. is a publisher of scientific and technical publications.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Sequence to Sequence Learning with Neural Networks">
      <data key="d0">Sequence to Sequence Learning with Neural Networks</data>
      <data key="d1">category</data>
      <data key="d2">Sequence to sequence learning is a neural network approach for tasks like machine translation, as described in the paper by Sutskever, Vinyals, and Le.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Rethinking the Inception Architecture for Computer Vision">
      <data key="d0">Rethinking the Inception Architecture for Computer Vision</data>
      <data key="d1">category</data>
      <data key="d2">Rethinking the Inception Architecture is the topic of a research paper by Szegedy et al., focusing on computer vision.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Computer Vision">
      <data key="d0">Computer Vision</data>
      <data key="d1">category</data>
      <data key="d2">Computer vision is a field of artificial intelligence focused on enabling computers to 'see' and interpret images.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Google’s Neural Machine Translation System">
      <data key="d0">Google’s Neural Machine Translation System</data>
      <data key="d1">category</data>
      <data key="d2">Google's Neural Machine Translation System is a machine translation system developed by Google, as described in the paper by Wu et al.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Human Translation">
      <data key="d0">Human Translation</data>
      <data key="d1">category</data>
      <data key="d2">Human translation is the process of translating text by a human translator.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d0">Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation</data>
      <data key="d1">category</data>
      <data key="d2">Deep Recurrent Models with Fast-Forward Connections is the topic of a research paper by Zhou et al., focusing on neural machine translation.</data>
      <data key="d3">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Self-Attention&quot;|&gt;&quot;The encoder contains self-attention layers to allow each position to attend to all positions in the previous layer.">
      <data key="d0">Self-Attention"|&gt;"The encoder contains self-attention layers to allow each position to attend to all positions in the previous layer.</data>
      <data key="d3">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d2">model component, attention type</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Self-Attention&quot;|&gt;&quot;The decoder contains self-attention layers to allow each position to attend to all positions up to and including that position.">
      <data key="d0">Self-Attention"|&gt;"The decoder contains self-attention layers to allow each position to attend to all positions up to and including that position.</data>
      <data key="d3">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d2">model component, attention type</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Overfitting">
      <data key="d0">Overfitting</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d2">Dropout is used to prevent overfitting.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">unknown_source</data>
    </node>
    <node id="Machine Reading">
      <data key="d0">Machine Reading</data>
      <data key="d3">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d2">Jianpeng Cheng is an author of a paper on machine reading.</data>
      <data key="d1">UNKNOWN</data>
      <data key="d4">unknown_source</data>
    </node>
    <edge source="Ashish Vaswani" target="Google Brain">
      <data key="d5">9.0</data>
      <data key="d6">Ashish Vaswani is affiliated with Google Brain.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ashish Vaswani" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Ashish Vaswani designed and implemented the first Transformer models.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="Google Brain">
      <data key="d5">9.0</data>
      <data key="d6">Noam Shazeer is affiliated with Google Brain.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Noam Shazeer proposed scaled dot-product attention, multi-head attention, and the parameter-free position representation for the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Noam Shazeer published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d5">8.0</data>
      <data key="d6">Noam Shazeer wrote a paper on Sparsely-Gated Mixture-of-Experts Layer.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="Jeff Dean">
      <data key="d5">9.0</data>
      <data key="d6">Both are co-authors of the same paper.</data>
      <data key="d7">collaboration, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Noam Shazeer" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Noam Shazeer is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Niki Parmar" target="Google Research">
      <data key="d5">9.0</data>
      <data key="d6">Niki Parmar is affiliated with Google Research.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Niki Parmar" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Niki Parmar designed, implemented, tuned and evaluated countless model variants for the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jakob Uszkoreit" target="Google Research">
      <data key="d5">9.0</data>
      <data key="d6">Jakob Uszkoreit is affiliated with Google Research.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jakob Uszkoreit" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Jakob Uszkoreit proposed replacing RNNs with self-attention, leading to the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jakob Uszkoreit" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Jakob Uszkoreit wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Llion Jones" target="Google Research">
      <data key="d5">9.0</data>
      <data key="d6">Llion Jones is affiliated with Google Research.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Llion Jones" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Llion Jones experimented with novel model variants and was responsible for the initial codebase for the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Aidan N. Gomez" target="University of Toronto">
      <data key="d5">9.0</data>
      <data key="d6">Aidan N. Gomez is affiliated with the University of Toronto.</data>
      <data key="d7">affiliation, education</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Aidan N. Gomez" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Aidan N. Gomez spent countless long days designing various parts of and implementing tensor2tensor for the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Łukasz Kaiser" target="Google Brain">
      <data key="d5">9.0</data>
      <data key="d6">Łukasz Kaiser is affiliated with Google Brain.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Łukasz Kaiser" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Łukasz Kaiser spent countless long days designing various parts of and implementing tensor2tensor for the Transformer.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Łukasz Kaiser" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Łukasz Kaiser published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Łukasz Kaiser" target="Neural GPUs">
      <data key="d5">8.0</data>
      <data key="d6">Łukasz Kaiser wrote a paper on Neural GPUs.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Łukasz Kaiser" target="Active Memory">
      <data key="d5">8.0</data>
      <data key="d6">Łukasz Kaiser wrote a paper on Active Memory.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Illia Polosukhin" target="Google Brain">
      <data key="d5">9.0</data>
      <data key="d6">Illia Polosukhin is affiliated with Google Brain.</data>
      <data key="d7">affiliation, employment</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Illia Polosukhin" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">Illia Polosukhin designed and implemented the first Transformer models.</data>
      <data key="d7">developer, contributor</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014 English-to-German Translation Task" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">The Transformer model achieved 28.4 BLEU on the WMT 2014 English-to-German translation task.</data>
      <data key="d7">performance, evaluation</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014 English-to-French Translation Task" target="Transformer">
      <data key="d5">10.0</data>
      <data key="d6">The Transformer model achieved a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task.</data>
      <data key="d7">performance, evaluation</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Long Beach, CA, USA" target="31st Conference on Neural Information Processing Systems (NIPS 2017)">
      <data key="d5">10.0</data>
      <data key="d6">The 31st Conference on Neural Information Processing Systems (NIPS 2017) took place in Long Beach, CA, USA.</data>
      <data key="d7">event location, conference venue</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Transformer">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer model is used for machine translation tasks.</data>
      <data key="d7">application, technology</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Nal Kalchbrenner">
      <data key="d5">8.0</data>
      <data key="d6">Nal Kalchbrenner wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Lasse Espeholt">
      <data key="d5">8.0</data>
      <data key="d6">Lasse Espeholt wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Karen Simonyan">
      <data key="d5">8.0</data>
      <data key="d6">Karen Simonyan wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Aaron van den Oord">
      <data key="d5">8.0</data>
      <data key="d6">Aaron van den Oord wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Koray Kavukcuoglu">
      <data key="d5">8.0</data>
      <data key="d6">Koray Kavukcuoglu wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Rico Sennrich">
      <data key="d5">8.0</data>
      <data key="d6">Rico Sennrich wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Barry Haddow">
      <data key="d5">8.0</data>
      <data key="d6">Barry Haddow wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Machine Translation" target="Alexandra Birch">
      <data key="d5">8.0</data>
      <data key="d6">Alexandra Birch wrote a paper on Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Attention Is All You Need">
      <data key="d5">10.0</data>
      <data key="d6">'Attention Is All You Need' is the name of the paper that introduces the Transformer model.</data>
      <data key="d7">title, introduction</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Sequence Transduction Models">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer is proposed as an alternative to existing sequence transduction models.</data>
      <data key="d7">replacement, alternative</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Recurrent Neural Networks">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer dispenses with recurrent neural networks.</data>
      <data key="d7">replacement, alternative</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Convolutional Neural Networks">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer dispenses with convolutional neural networks.</data>
      <data key="d7">replacement, alternative</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Attention Mechanism">
      <data key="d5">10.0</data>
      <data key="d6">The Transformer is based solely on attention mechanisms.</data>
      <data key="d7">foundation, core component</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Machine Translation Tasks">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer is evaluated on machine translation tasks.</data>
      <data key="d7">application, evaluation</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="BLEU">
      <data key="d5">7.0</data>
      <data key="d6">BLEU is used to measure the performance of the Transformer.</data>
      <data key="d7">metric, evaluation</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="P100 GPUs">
      <data key="d5">7.0</data>
      <data key="d6">P100 GPUs were used to train the Transformer model.</data>
      <data key="d7">hardware, training</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Self-Attention">
      <data key="d5">18.0</data>
      <data key="d6">Self-attention is an important part of the Transformer model.&lt;SEP&gt;The Transformer model relies entirely on self-attention to compute representations.</data>
      <data key="d7">component, mechanism&lt;SEP&gt;model architecture, attention mechanism</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Multi-Head Attention">
      <data key="d5">18.0</data>
      <data key="d6">Multi-Head Attention is used in the Transformer to counteract reduced effective resolution.&lt;SEP&gt;The Transformer model uses multi-head attention in its architecture.</data>
      <data key="d7">component, mechanism&lt;SEP&gt;model architecture, attention mechanism</data>
      <data key="d8">chunk-e62c8746888f81ea381fd8f2a131ba4a&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="RNNs">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer is designed to avoid using sequence-aligned RNNs, relying instead on self-attention.</data>
      <data key="d7">model architecture, alternative approach</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Convolution">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer is designed to avoid using convolution, relying instead on self-attention.</data>
      <data key="d7">model architecture, alternative approach</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Positional Encoding">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer model uses positional encoding to incorporate information about the order of the sequence.</data>
      <data key="d7">model architecture, sequence information</data>
      <data key="d8">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="English-to-German Translation">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer model's performance is evaluated on the English-to-German translation task.</data>
      <data key="d7">model evaluation, translation task</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Attention Heads">
      <data key="d5">7.0</data>
      <data key="d6">The number of attention heads is varied to evaluate its impact on the Transformer's performance.</data>
      <data key="d7">model architecture, performance tuning</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="GPUs">
      <data key="d5">9.0</data>
      <data key="d6">GPUs are used to train the Transformer model.</data>
      <data key="d7">model training, hardware resources</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="WMT 2014 English-to-German">
      <data key="d5">10.0</data>
      <data key="d6">The Transformer model achieved state-of-the-art results on the WMT 2014 English-to-German translation task.</data>
      <data key="d7">performance benchmark, translation task</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="WMT 2014 English-to-French">
      <data key="d5">10.0</data>
      <data key="d6">The Transformer model achieved state-of-the-art results on the WMT 2014 English-to-French translation task.</data>
      <data key="d7">performance benchmark, translation task</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Nal Kalchbrenner">
      <data key="d5">5.0</data>
      <data key="d6">Nal Kalchbrenner is acknowledged for contributions to the Transformer project.</data>
      <data key="d7">acknowledgment, contribution</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Stephan Gouws">
      <data key="d5">5.0</data>
      <data key="d6">Stephan Gouws is acknowledged for contributions to the Transformer project.</data>
      <data key="d7">acknowledgment, contribution</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Transformer" target="Model Variations">
      <data key="d5">8.0</data>
      <data key="d6">Model variations are tested to evaluate the importance of different components of the Transformer.</data>
      <data key="d7">model architecture, experimentation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Recurrent Neural Networks" target="Alex Graves">
      <data key="d5">8.0</data>
      <data key="d6">Alex Graves wrote a paper on Recurrent Neural Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Encoder" target="Decoder">
      <data key="d5">28.0</data>
      <data key="d6">The Encoder and Decoder are the two main components of the Transformer model's architecture.&lt;SEP&gt;The encoder and decoder are both parts of the Transformer model and work together to process input and generate output.&lt;SEP&gt;The encoder and decoder are the two main components of a sequence transduction model.</data>
      <data key="d7">model architecture, sequence processing&lt;SEP&gt;model architecture, sequence transduction</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59&lt;SEP&gt;chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Encoder" target="Layer Normalization">
      <data key="d5">6.0</data>
      <data key="d6">Layer normalization is applied to the sub-layers within the encoder to improve training.</data>
      <data key="d7">model component, training improvement</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Encoder" target="Self-Attention&quot;|&gt;&quot;The encoder contains self-attention layers to allow each position to attend to all positions in the previous layer.">
      <data key="d5">9.0</data>
      <data key="d6">model component, attention type</data>
      <data key="d7">9</data>
      <data key="d8">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Decoder" target="Layer Normalization">
      <data key="d5">6.0</data>
      <data key="d6">Layer normalization is applied to the sub-layers within the decoder to improve training.</data>
      <data key="d7">model component, training improvement</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Decoder" target="Self-Attention&quot;|&gt;&quot;The decoder contains self-attention layers to allow each position to attend to all positions up to and including that position.">
      <data key="d5">9.0</data>
      <data key="d6">model component, attention type</data>
      <data key="d7">9</data>
      <data key="d8">chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="BLEU" target="Transformer (base model)">
      <data key="d5">9.0</data>
      <data key="d6">BLEU score is used to evaluate the performance of the base Transformer model.</data>
      <data key="d7">evaluation metric, performance</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="BLEU" target="Transformer (big)">
      <data key="d5">9.0</data>
      <data key="d6">BLEU score is used to evaluate the performance of the big Transformer model.</data>
      <data key="d7">evaluation metric, performance</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="BLEU" target="Single-Head Attention">
      <data key="d5">8.0</data>
      <data key="d6">Single-head attention results in worse BLEU scores compared to multi-headed attention.</data>
      <data key="d7">performance metric, attention mechanism</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Long Short-Term Memory" target="Sepp Hochreiter">
      <data key="d5">8.0</data>
      <data key="d6">Sepp Hochreiter wrote a paper on Long Short-Term Memory.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Long Short-Term Memory" target="Jürgen Schmidhuber">
      <data key="d5">8.0</data>
      <data key="d6">Jürgen Schmidhuber wrote a paper on Long Short-Term Memory.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Language Modeling" target="Rafal Jozefowicz">
      <data key="d5">8.0</data>
      <data key="d6">Rafal Jozefowicz wrote a paper on Language Modeling.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Language Modeling" target="Oriol Vinyals">
      <data key="d5">8.0</data>
      <data key="d6">Oriol Vinyals wrote a paper on Language Modeling.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Language Modeling" target="Mike Schuster">
      <data key="d5">8.0</data>
      <data key="d6">Mike Schuster wrote a paper on Language Modeling.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Encoder-Decoder Architectures" target="Multi-Headed Self-Attention">
      <data key="d5">8.0</data>
      <data key="d6">Multi-headed self-attention replaces recurrent layers in encoder-decoder architectures.</data>
      <data key="d7">model architecture, attention mechanism</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="Multi-Head Attention">
      <data key="d5">7.0</data>
      <data key="d6">Multi-Head Attention is a refined version of Self-Attention used within the Transformer architecture.</data>
      <data key="d7">attention mechanism, refinement</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="End-To-End Memory Networks">
      <data key="d5">7.0</data>
      <data key="d6">End-to-end memory networks use recurrent attention, while the Transformer uses self-attention.</data>
      <data key="d7">attention mechanism, architectural difference</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="WMT 2014 English-German Dataset">
      <data key="d5">7.0</data>
      <data key="d6">Self-attention mechanisms are used in models trained on the WMT 2014 English-German dataset.</data>
      <data key="d7">training data, model application</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="WMT 2014 English-French Dataset">
      <data key="d5">7.0</data>
      <data key="d6">Self-attention mechanisms are used in models trained on the WMT 2014 English-French dataset.</data>
      <data key="d7">training data, model application</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="NVIDIA P100 GPUs">
      <data key="d5">8.0</data>
      <data key="d6">NVIDIA P100 GPUs are used for training models that utilize self-attention mechanisms.</data>
      <data key="d7">hardware, model training</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="Convolutional Layers">
      <data key="d5">7.0</data>
      <data key="d6">Convolutional layers are compared to self-attention layers in terms of computational complexity and path length.</data>
      <data key="d7">comparison, computational efficiency</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Self-Attention" target="Recurrent Layers">
      <data key="d5">7.0</data>
      <data key="d6">Recurrent layers are compared to self-attention layers in terms of computational complexity and path length.</data>
      <data key="d7">comparison, computational efficiency</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="ByteNet" target="WMT 2014">
      <data key="d5">7.0</data>
      <data key="d6">ByteNet is a model that competed in the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="ConvS2S" target="WMT 2014">
      <data key="d5">7.0</data>
      <data key="d6">ConvS2S is a model that competed in the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Multi-Head Attention" target="Q">
      <data key="d5">15.0</data>
      <data key="d6">Multi-Head Attention uses a matrix of queries Q as input to the attention mechanism.&lt;SEP&gt;Multi-head attention uses queries (Q) as one of its inputs.</data>
      <data key="d7">attention mechanism, input data&lt;SEP&gt;attention mechanism, input parameters</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Multi-Head Attention" target="K">
      <data key="d5">15.0</data>
      <data key="d6">Multi-Head Attention uses a matrix of keys K as input to the attention mechanism.&lt;SEP&gt;Multi-head attention uses keys (K) as one of its inputs.</data>
      <data key="d7">attention mechanism, input data&lt;SEP&gt;attention mechanism, input parameters</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Multi-Head Attention" target="V">
      <data key="d5">15.0</data>
      <data key="d6">Multi-Head Attention uses a matrix of values V as input to the attention mechanism.&lt;SEP&gt;Multi-head attention uses values (V) as one of its inputs.</data>
      <data key="d7">attention mechanism, input data&lt;SEP&gt;attention mechanism, input parameters</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0&lt;SEP&gt;chunk-71e5989854dc53ec9274d4aad2f3cd59</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Abstractive Summarization" target="Romain Paulus">
      <data key="d5">8.0</data>
      <data key="d6">Romain Paulus wrote a paper on Abstractive Summarization.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Abstractive Summarization" target="Caiming Xiong">
      <data key="d5">8.0</data>
      <data key="d6">Caiming Xiong wrote a paper on Abstractive Summarization.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Abstractive Summarization" target="Richard Socher">
      <data key="d5">8.0</data>
      <data key="d6">Richard Socher wrote a paper on Abstractive Summarization.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Scaled Dot-Product Attention" target="Dot-Product Attention">
      <data key="d5">8.0</data>
      <data key="d6">Scaled Dot-Product Attention is a modified version of dot-product attention with a scaling factor.</data>
      <data key="d7">attention mechanism, scaling</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Additive Attention" target="Dot-Product Attention">
      <data key="d5">7.0</data>
      <data key="d6">Additive attention and dot-product attention are two different methods for computing attention, with different computational properties.</data>
      <data key="d7">attention mechanism, alternative approach</data>
      <data key="d8">chunk-99b697e08b892dd6490f4d00848f89f0</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Layer Normalization" target="Jimmy Lei Ba">
      <data key="d5">5.0</data>
      <data key="d6">Jimmy Lei Ba is an author of a paper on layer normalization.</data>
      <data key="d7">research, citation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014 English-German Dataset" target="English-German Dataset">
      <data key="d5">8.0</data>
      <data key="d6">The WMT 2014 English-German Dataset is a prominent dataset.</data>
      <data key="d7">dataset, translation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014 English-French Dataset" target="English-French Dataset">
      <data key="d5">8.0</data>
      <data key="d6">The WMT 2014 English-French Dataset is a prominent dataset.</data>
      <data key="d7">dataset, translation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="NVIDIA P100 GPUs" target="NVIDIA">
      <data key="d5">8.0</data>
      <data key="d6">NVIDIA P100 GPUs is a product of NVIDIA.</data>
      <data key="d7">hardware, company</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Positional Encodings" target="Sine and Cosine Functions">
      <data key="d5">9.0</data>
      <data key="d6">Sine and cosine functions are used to create positional encodings.</data>
      <data key="d7">encoding method, position representation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Word-Piece" target="Byte-Pair">
      <data key="d5">7.0</data>
      <data key="d6">Word-piece and byte-pair representations are both tokenization methods used in machine translation.</data>
      <data key="d7">tokenization, representation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Convolutional Layers" target="Recurrent Layers">
      <data key="d5">7.0</data>
      <data key="d6">Recurrent and convolutional layers are replaced with multi-headed self-attention in the Transformer model.</data>
      <data key="d7">model architecture, attention mechanism</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="English-German Dataset">
      <data key="d5">8.0</data>
      <data key="d6">The WMT 2014 provides the English-German Dataset.</data>
      <data key="d7">dataset provider, translation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="English-French Dataset">
      <data key="d5">8.0</data>
      <data key="d6">The WMT 2014 provides the English-French Dataset.</data>
      <data key="d7">dataset provider, translation</data>
      <data key="d8">chunk-51ac0ead3179a6c23fe27aa141b5d58b</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="GNMT + RL">
      <data key="d5">7.0</data>
      <data key="d6">GNMT + RL is a model that competed in the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="MoE">
      <data key="d5">7.0</data>
      <data key="d6">MoE is a model that competed in the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="Transformer (base model)">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer models were evaluated on the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="Transformer (big)">
      <data key="d5">8.0</data>
      <data key="d6">The Transformer models were evaluated on the WMT 2014 translation task.</data>
      <data key="d7">competition, evaluation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="English-to-German Translation">
      <data key="d5">9.0</data>
      <data key="d6">English-to-German translation is one of the tasks in the WMT 2014 competition.</data>
      <data key="d7">task, competition</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014" target="English-to-French Translation">
      <data key="d5">9.0</data>
      <data key="d6">English-to-French translation is one of the tasks in the WMT 2014 competition.</data>
      <data key="d7">task, competition</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="NVIDIA" target="P100">
      <data key="d5">10.0</data>
      <data key="d6">The models were trained on NVIDIA P100 GPUs.</data>
      <data key="d7">hardware, training</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="P100" target="Transformer (base model)">
      <data key="d5">8.0</data>
      <data key="d6">P100 GPUs are used to train the Transformer base model.</data>
      <data key="d7">hardware, training</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="P100" target="Transformer (big)">
      <data key="d5">8.0</data>
      <data key="d6">P100 GPUs are used to train the Transformer big model.</data>
      <data key="d7">hardware, training</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Adam" target="Transformer (base model)">
      <data key="d5">10.0</data>
      <data key="d6">The Adam optimizer was used to train the Transformer models.</data>
      <data key="d7">optimization, training</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="English-to-German Translation" target="Newstest2014">
      <data key="d5">8.0</data>
      <data key="d6">Newstest2014 is used as a test set for the English-to-German translation task.</data>
      <data key="d7">evaluation, test set</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="English-to-German Translation" target="Newstest2013">
      <data key="d5">17.0</data>
      <data key="d6">Newstest2013 is the development set used to measure the performance of English-to-German translation.&lt;SEP&gt;Newstest2013 is used as a development set for the English-to-German translation task.</data>
      <data key="d7">dataset, evaluation metric&lt;SEP&gt;evaluation, development set</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e&lt;SEP&gt;chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="English-to-French Translation" target="Newstest2014">
      <data key="d5">8.0</data>
      <data key="d6">Newstest2014 is used as a test set for the English-to-French translation task.</data>
      <data key="d7">evaluation, test set</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Source Tokens" target="Target Tokens">
      <data key="d5">9.0</data>
      <data key="d6">The source and target tokens represent the input and output of the translation models.</data>
      <data key="d7">input-output, translation</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Base Models" target="Big Models">
      <data key="d5">8.0</data>
      <data key="d6">The base models are smaller and faster to train compared to the big models.</data>
      <data key="d7">model size, training time</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Base Models" target="Warmup Steps">
      <data key="d5">7.0</data>
      <data key="d6">Warmup steps are used during the training of the base models to stabilize learning.</data>
      <data key="d7">training phase, learning rate</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Base Models" target="Residual Dropout">
      <data key="d5">8.0</data>
      <data key="d6">Residual dropout is applied to the base models to prevent overfitting.</data>
      <data key="d7">regularization, overfitting</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Base Models" target="Label Smoothing">
      <data key="d5">8.0</data>
      <data key="d6">Label smoothing is employed during the training of the base models to improve accuracy.</data>
      <data key="d7">regularization, accuracy</data>
      <data key="d8">chunk-99f2eaaef68a78d7413de4d393e0a7f4</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="GPUs" target="Floating Point Operations">
      <data key="d5">9.0</data>
      <data key="d6">GPUs perform floating point operations to train the model.</data>
      <data key="d7">hardware acceleration, computation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="WMT 2014 English-to-German" target="WMT 2014 English-to-French">
      <data key="d5">9.0</data>
      <data key="d6">The Transformer achieved state-of-the-art results on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.</data>
      <data key="d7">translation tasks, performance benchmark</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Nal Kalchbrenner" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Nal Kalchbrenner published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Attention Key" target="Attention Value Dimensions">
      <data key="d5">7.0</data>
      <data key="d6">The attention key and value dimensions are varied to evaluate their impact on model performance.</data>
      <data key="d7">model architecture, attention mechanism</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Perplexity" target="Byte-Pair Encoding">
      <data key="d5">7.0</data>
      <data key="d6">Perplexity is measured per-wordpiece according to byte-pair encoding.</data>
      <data key="d7">text encoding, evaluation metric</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Overfitting">
      <data key="d5">9.0</data>
      <data key="d6">Dropout is used to prevent overfitting.</data>
      <data key="d7">regularization, model training</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Nitish Srivastava">
      <data key="d5">8.0</data>
      <data key="d6">Nitish Srivastava is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Geoffrey Hinton">
      <data key="d5">8.0</data>
      <data key="d6">Geoffrey Hinton is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Alex Krizhevsky">
      <data key="d5">8.0</data>
      <data key="d6">Alex Krizhevsky is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Ilya Sutskever">
      <data key="d5">8.0</data>
      <data key="d6">Ilya Sutskever is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dropout" target="Ruslan Salakhutdinov">
      <data key="d5">8.0</data>
      <data key="d6">Ruslan Salakhutdinov is an author of a paper on dropout regularization in neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Sinusoidal Positional Encoding" target="Learned Positional Embeddings">
      <data key="d5">6.0</data>
      <data key="d6">Learned positional embeddings are a replacement for sinusoidal positional encoding.</data>
      <data key="d7">encoding method, model architecture</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Neural Machine Translation" target="Dzmitry Bahdanau">
      <data key="d5">5.0</data>
      <data key="d6">Dzmitry Bahdanau is an author of a paper on neural machine translation.</data>
      <data key="d7">research, citation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Neural Machine Translation" target="Denny Britz">
      <data key="d5">5.0</data>
      <data key="d6">Denny Britz is an author of a paper on neural machine translation architectures.</data>
      <data key="d7">research, citation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Kyunghyun Cho" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Kyunghyun Cho published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Kyunghyun Cho" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Kyunghyun Cho wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Kyunghyun Cho" target="Gated Recurrent Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Kyunghyun Cho wrote a paper on Gated Recurrent Neural Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoshua Bengio" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Yoshua Bengio published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoshua Bengio" target="Junyoung Chung">
      <data key="d5">6.0</data>
      <data key="d6">Yoshua Bengio and Junyoung Chung co-authored a paper.</data>
      <data key="d7">collaboration, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoshua Bengio" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Yoshua Bengio wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoshua Bengio" target="Gated Recurrent Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Yoshua Bengio wrote a paper on Gated Recurrent Neural Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Denny Britz" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Denny Britz published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Denny Britz" target="Neural Machine Translation Architectures">
      <data key="d5">8.0</data>
      <data key="d6">Denny Britz wrote a paper on Neural Machine Translation Architectures.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Anna Goldie" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Anna Goldie published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Anna Goldie" target="Neural Machine Translation Architectures">
      <data key="d5">8.0</data>
      <data key="d6">Anna Goldie wrote a paper on Neural Machine Translation Architectures.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Minh-Thang Luong" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Minh-Thang Luong published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Minh-Thang Luong" target="Neural Machine Translation Architectures">
      <data key="d5">8.0</data>
      <data key="d6">Minh-Thang Luong wrote a paper on Neural Machine Translation Architectures.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Minh-Thang Luong" target="Attention-Based Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Minh-Thang Luong wrote a paper on Attention-Based Neural Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc V. Le" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Quoc V. Le published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc V. Le" target="Neural Machine Translation Architectures">
      <data key="d5">8.0</data>
      <data key="d6">Quoc V. Le wrote a paper on Neural Machine Translation Architectures.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jianpeng Cheng" target="Machine Reading">
      <data key="d5">5.0</data>
      <data key="d6">Jianpeng Cheng is an author of a paper on machine reading.</data>
      <data key="d7">research, citation</data>
      <data key="d8">chunk-ee892a85e6aa64d4d507385a3ad9a72e</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jianpeng Cheng" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Jianpeng Cheng published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jianpeng Cheng" target="Long Short-Term Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Jianpeng Cheng wrote a paper on Long Short-Term Memory Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Li Dong" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Li Dong published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Li Dong" target="Long Short-Term Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Li Dong wrote a paper on Long Short-Term Memory Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mirella Lapata" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Mirella Lapata published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mirella Lapata" target="Long Short-Term Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Mirella Lapata wrote a paper on Long Short-Term Memory Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bart van Merrienboer" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Bart van Merrienboer published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bart van Merrienboer" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Bart van Merrienboer wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Caglar Gulcehre" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Caglar Gulcehre published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Caglar Gulcehre" target="Junyoung Chung">
      <data key="d5">6.0</data>
      <data key="d6">Caglar Gulcehre and Junyoung Chung co-authored a paper.</data>
      <data key="d7">collaboration, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Caglar Gulcehre" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Caglar Gulcehre wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Caglar Gulcehre" target="Gated Recurrent Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Caglar Gulcehre wrote a paper on Gated Recurrent Neural Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Fethi Bougares" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Fethi Bougares published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Fethi Bougares" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Fethi Bougares wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Holger Schwenk" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Holger Schwenk published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Holger Schwenk" target="RNN Encoder-Decoder">
      <data key="d5">8.0</data>
      <data key="d6">Holger Schwenk wrote a paper on RNN Encoder-Decoder.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Francois Chollet" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Francois Chollet published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Francois Chollet" target="Depthwise Separable Convolutions">
      <data key="d5">8.0</data>
      <data key="d6">Francois Chollet wrote a paper on Depthwise Separable Convolutions.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Junyoung Chung" target="CoRR">
      <data key="d5">7.0</data>
      <data key="d6">Junyoung Chung published a paper in CoRR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jonas Gehring" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Jonas Gehring published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jonas Gehring" target="Sequence to Sequence Learning">
      <data key="d5">8.0</data>
      <data key="d6">Jonas Gehring wrote a paper on Sequence to Sequence Learning.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Michael Auli" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Michael Auli published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Michael Auli" target="Sequence to Sequence Learning">
      <data key="d5">8.0</data>
      <data key="d6">Michael Auli wrote a paper on Sequence to Sequence Learning.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="David Grangier" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">David Grangier published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="David Grangier" target="Sequence to Sequence Learning">
      <data key="d5">8.0</data>
      <data key="d6">David Grangier wrote a paper on Sequence to Sequence Learning.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Denis Yarats" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Denis Yarats published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Denis Yarats" target="Sequence to Sequence Learning">
      <data key="d5">8.0</data>
      <data key="d6">Denis Yarats wrote a paper on Sequence to Sequence Learning.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yann N. Dauphin" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Yann N. Dauphin published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yann N. Dauphin" target="Sequence to Sequence Learning">
      <data key="d5">8.0</data>
      <data key="d6">Yann N. Dauphin wrote a paper on Sequence to Sequence Learning.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Alex Graves" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Alex Graves published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Kaiming He" target="IEEE">
      <data key="d5">7.0</data>
      <data key="d6">Kaiming He published a paper in IEEE.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Kaiming He" target="Image Recognition">
      <data key="d5">8.0</data>
      <data key="d6">Kaiming He wrote a paper on Image Recognition.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Xiangyu Zhang" target="IEEE">
      <data key="d5">7.0</data>
      <data key="d6">Xiangyu Zhang published a paper in IEEE.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Xiangyu Zhang" target="Image Recognition">
      <data key="d5">8.0</data>
      <data key="d6">Xiangyu Zhang wrote a paper on Image Recognition.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Shaoqing Ren" target="IEEE">
      <data key="d5">7.0</data>
      <data key="d6">Shaoqing Ren published a paper in IEEE.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Shaoqing Ren" target="Image Recognition">
      <data key="d5">8.0</data>
      <data key="d6">Shaoqing Ren wrote a paper on Image Recognition.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jian Sun" target="IEEE">
      <data key="d5">7.0</data>
      <data key="d6">Jian Sun published a paper in IEEE.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jian Sun" target="Image Recognition">
      <data key="d5">8.0</data>
      <data key="d6">Jian Sun wrote a paper on Image Recognition.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Sepp Hochreiter" target="Jürgen Schmidhuber">
      <data key="d5">6.0</data>
      <data key="d6">Sepp Hochreiter and Jürgen Schmidhuber co-authored a paper.</data>
      <data key="d7">collaboration, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Rafal Jozefowicz" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Rafal Jozefowicz published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oriol Vinyals" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Oriol Vinyals published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oriol Vinyals" target="Sequence to Sequence Learning with Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Oriol Vinyals is an author of a paper on sequence to sequence learning.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mike Schuster" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Mike Schuster published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mike Schuster" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Mike Schuster is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yonghui Wu" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Yonghui Wu published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yonghui Wu" target="Google">
      <data key="d5">7.0</data>
      <data key="d6">Yonghui Wu is an author of Google's neural machine translation system paper.</data>
      <data key="d7">research, affiliation</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yonghui Wu" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Yonghui Wu is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ilya Sutskever" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Ilya Sutskever published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ilya Sutskever" target="Neural GPUs">
      <data key="d5">8.0</data>
      <data key="d6">Ilya Sutskever wrote a paper on Neural GPUs.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ilya Sutskever" target="Sequence to Sequence Learning with Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Ilya Sutskever is an author of a paper on sequence to sequence learning.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Lasse Espeholt" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Lasse Espeholt published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Karen Simonyan" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Karen Simonyan published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Aaron van den Oord" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Aaron van den Oord published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Koray Kavukcuoglu" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Koray Kavukcuoglu published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoon Kim" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Yoon Kim published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yoon Kim" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Yoon Kim wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Carl Denton" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Carl Denton published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Carl Denton" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Carl Denton wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Luong Hoang" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Luong Hoang published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Luong Hoang" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Luong Hoang wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Alexander M. Rush" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Alexander M. Rush published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Alexander M. Rush" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Alexander M. Rush wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Diederik Kingma" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Diederik Kingma published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Diederik Kingma" target="Stochastic Optimization">
      <data key="d5">8.0</data>
      <data key="d6">Diederik Kingma wrote a paper on Stochastic Optimization.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jimmy Ba" target="ICLR">
      <data key="d5">7.0</data>
      <data key="d6">Jimmy Ba published a paper in ICLR.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jimmy Ba" target="Stochastic Optimization">
      <data key="d5">8.0</data>
      <data key="d6">Jimmy Ba wrote a paper on Stochastic Optimization.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oleksii Kuchaiev" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Oleksii Kuchaiev published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oleksii Kuchaiev" target="LSTM Networks">
      <data key="d5">8.0</data>
      <data key="d6">Oleksii Kuchaiev wrote a paper on LSTM Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Boris Ginsburg" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Boris Ginsburg published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Boris Ginsburg" target="LSTM Networks">
      <data key="d5">8.0</data>
      <data key="d6">Boris Ginsburg wrote a paper on LSTM Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Zhouhan Lin" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Zhouhan Lin published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Zhouhan Lin" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Zhouhan Lin wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Minwei Feng" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Minwei Feng published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Minwei Feng" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Minwei Feng wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Cicero Nogueira dos Santos" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Cicero Nogueira dos Santos published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Cicero Nogueira dos Santos" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Cicero Nogueira dos Santos wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mo Yu" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Mo Yu published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mo Yu" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Mo Yu wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bing Xiang" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Bing Xiang published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bing Xiang" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Bing Xiang wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bowen Zhou" target="arXiv">
      <data key="d5">7.0</data>
      <data key="d6">Bowen Zhou published a paper in arXiv.</data>
      <data key="d7">publication, research</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Bowen Zhou" target="Sentence Embedding">
      <data key="d5">8.0</data>
      <data key="d6">Bowen Zhou wrote a paper on Sentence Embedding.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Samy Bengio" target="Active Memory">
      <data key="d5">8.0</data>
      <data key="d6">Samy Bengio wrote a paper on Active Memory.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Hieu Pham" target="Attention-Based Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Hieu Pham wrote a paper on Attention-Based Neural Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Christopher D Manning" target="Attention-Based Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Christopher D Manning wrote a paper on Attention-Based Neural Machine Translation.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ankur Parikh" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Ankur Parikh wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oscar Täckström" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Oscar Täckström wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Dipanjan Das" target="Attention Networks">
      <data key="d5">8.0</data>
      <data key="d6">Dipanjan Das wrote a paper on Attention Networks.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Oﬁr Press" target="Language Models">
      <data key="d5">8.0</data>
      <data key="d6">Oﬁr Press wrote a paper on Language Models.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Lior Wolf" target="Language Models">
      <data key="d5">8.0</data>
      <data key="d6">Lior Wolf wrote a paper on Language Models.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Azalia Mirhoseini" target="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d5">8.0</data>
      <data key="d6">Azalia Mirhoseini wrote a paper on Sparsely-Gated Mixture-of-Experts Layer.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Azalia Mirhoseini" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Azalia Mirhoseini is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Krzysztof Maziarz" target="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d5">8.0</data>
      <data key="d6">Krzysztof Maziarz wrote a paper on Sparsely-Gated Mixture-of-Experts Layer.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Krzysztof Maziarz" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Krzysztof Maziarz is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Andy Davis" target="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d5">8.0</data>
      <data key="d6">Andy Davis wrote a paper on Sparsely-Gated Mixture-of-Experts Layer.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Andy Davis" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Andy Davis is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Geoffrey Hinton" target="Sparsely-Gated Mixture-of-Experts Layer">
      <data key="d5">8.0</data>
      <data key="d6">Geoffrey Hinton wrote a paper on Sparsely-Gated Mixture-of-Experts Layer.</data>
      <data key="d7">research, publication</data>
      <data key="d8">chunk-9a840306c0ad1b3583cae839dafb5c28</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Geoffrey Hinton" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Geoffrey Hinton is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jeff Dean" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Jeff Dean is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc Le" target="Google">
      <data key="d5">7.0</data>
      <data key="d6">Quoc Le is an author on Google's neural machine translation system.</data>
      <data key="d7">research, affiliation</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc Le" target="Outrageously Large Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Quoc Le is an author of a paper on outrageously large neural networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc Le" target="Sequence to Sequence Learning with Neural Networks">
      <data key="d5">8.0</data>
      <data key="d6">Quoc Le is an author of a paper on sequence to sequence learning.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Quoc Le" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Quoc Le is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Sainbayar Sukhbaatar" target="End-to-End Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Sainbayar Sukhbaatar is an author of a paper on end-to-end memory networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Arthur Szlam" target="End-to-End Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Arthur Szlam is an author of a paper on end-to-end memory networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jason Weston" target="End-to-End Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Jason Weston is an author of a paper on end-to-end memory networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Rob Fergus" target="End-to-End Memory Networks">
      <data key="d5">8.0</data>
      <data key="d6">Rob Fergus is an author of a paper on end-to-end memory networks.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Christian Szegedy" target="Rethinking the Inception Architecture for Computer Vision">
      <data key="d5">8.0</data>
      <data key="d6">Christian Szegedy is an author of a paper on rethinking the inception architecture for computer vision.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Vincent Vanhoucke" target="Rethinking the Inception Architecture for Computer Vision">
      <data key="d5">8.0</data>
      <data key="d6">Vincent Vanhoucke is an author of a paper on rethinking the inception architecture for computer vision.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Sergey Ioffe" target="Rethinking the Inception Architecture for Computer Vision">
      <data key="d5">8.0</data>
      <data key="d6">Sergey Ioffe is an author of a paper on rethinking the inception architecture for computer vision.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jonathon Shlens" target="Rethinking the Inception Architecture for Computer Vision">
      <data key="d5">8.0</data>
      <data key="d6">Jonathon Shlens is an author of a paper on rethinking the inception architecture for computer vision.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Zbigniew Wojna" target="Rethinking the Inception Architecture for Computer Vision">
      <data key="d5">8.0</data>
      <data key="d6">Zbigniew Wojna is an author of a paper on rethinking the inception architecture for computer vision.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Zhifeng Chen" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Zhifeng Chen is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Mohammad Norouzi" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Mohammad Norouzi is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Wolfgang Macherey" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Wolfgang Macherey is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Maxim Krikun" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Maxim Krikun is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Yuan Cao" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Yuan Cao is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Qin Gao" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Qin Gao is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Klaus Macherey" target="Google’s Neural Machine Translation System">
      <data key="d5">8.0</data>
      <data key="d6">Klaus Macherey is an author of a paper on Google's neural machine translation system.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Jie Zhou" target="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Jie Zhou is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Ying Cao" target="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Ying Cao is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Xuguang Wang" target="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Xuguang Wang is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Peng Li" target="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Peng Li is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Wei Xu" target="Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation">
      <data key="d5">8.0</data>
      <data key="d6">Wei Xu is an author of a paper on deep recurrent models for neural machine translation.</data>
      <data key="d7">research, authorship</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
    <edge source="Advances in Neural Information Processing Systems" target="Curran Associates, Inc.">
      <data key="d5">7.0</data>
      <data key="d6">Curran Associates, Inc. publishes the proceedings of the Advances in Neural Information Processing Systems conference.</data>
      <data key="d7">publication, conference proceedings</data>
      <data key="d8">chunk-ea71515ce0be7fcc474ebb882dbcc33f</data>
      <data key="d9">unknown_source</data>
    </edge>
  </graph>
</graphml>
